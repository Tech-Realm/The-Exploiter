import os.path
import pandas as pd
import requests
from bs4 import BeautifulSoup

if os.path.exists("urls.xlsx"):
    df = pd.read_excel("urls.xlsx")
    urls = df['URL'].tolist()
else:
    urls = []
    google_dork = 'site:mit.edu inurl:index of'
    num_results = 10 #number of result per page
    start = 0
    while start<100: # number of pages you want to scrap
        search_url = f'https://www.google.com/search?q={google_dork}&num={num_results}&start={start}'
        res = requests.get(search_url)
        soup = BeautifulSoup(res.text, 'html.parser')
        for a in soup.select('a[href^="/url"]'):
            url = a['href'][7:]
            if url.startswith('http'):
                urls.append(url)
        start += num_results
    df = pd.DataFrame(columns=['URL'])

# Iterate through the URLs and save them to the sheet
for url in urls:
    df = df.append({'URL': url}, ignore_index=True)

# Save the dataframe to an excel file
df.to_excel("urls.xlsx")
